---
title: "Lab 3: Linear Regression (18/09/2024)"
format:
  pdf:
    toc: true
    number-sections: true
    colorlinks: true
  html:
    code-fold: false
jupyter: python3
author: "Matthew Loh"
---

```{python}
import numpy as np
import matplotlib.pyplot as plt
```
```{python}
# In this part of this exercise, we will implement linear regression with one
# variable to predict profits for a food truck. Suppose you are the CEO of a
# restaurant franchise and are considering different cities for opening a new
# outlet. The chain already has trucks in various cities and you have data for
# profits and populations from the cities. You would like to use this data to
# help you select which city to expand to next.

data = [
    (6.1101, 17.592),
    (5.5277, 9.1302),
    (8.5186, 13.662),
    (7.0032, 11.854),
    (5.8598, 6.8233),
    (8.3829, 11.886),
    (7.4764, 4.3483),
    (8.5781, 12),
    (6.4862, 6.5987),
    (5.0546, 3.8166),
    (5.7107, 3.2522),
    (14.164, 15.505),
    (5.734, 3.1551),
    (8.4084, 7.2258),
    (5.6407, 0.71618),
    (5.3794, 3.5129),
    (6.3654, 5.3048),
    (5.1301, 0.56077),
    (6.4296, 3.6518),
    (7.0708, 5.3893),
    (6.1891, 3.1386),
    (20.27, 21.767),
    (5.4901, 4.263),
    (6.3261, 5.1875),
    (5.5649, 3.0825),
    (18.945, 22.638),
    (12.828, 13.501),
    (10.957, 7.0467),
    (13.176, 14.692),
    (22.203, 24.147),
    (5.2524, -1.22),
    (6.5894, 5.9966),
    (9.2482, 12.134),
    (5.8918, 1.8495),
    (8.2111, 6.5426),
    (7.9334, 4.5623),
    (8.0959, 4.1164),
    (5.6063, 3.3928),
    (12.836, 10.117),
    (6.3534, 5.4974),
    (5.4069, 0.55657),
    (6.8825, 3.9115),
    (11.708, 5.3854),
    (5.7737, 2.4406),
    (7.8247, 6.7318),
    (7.0931, 1.0463),
    (5.0702, 5.1337),
    (5.8014, 1.844),
    (11.7, 8.0043),
    (5.5416, 1.0179),
    (7.5402, 6.7504),
    (5.3077, 1.8396),
    (7.4239, 4.2885),
    (7.6031, 4.9981),
    (6.3328, 1.4233),
    (6.3589, -1.4211),
    (6.2742, 2.4756),
    (5.6397, 4.6042),
    (9.3102, 3.9624),
    (9.4536, 5.4141),
    (8.8254, 5.1694),
    (5.1793, -0.74279),
    (21.279, 17.929),
    (14.908, 12.054),
    (18.959, 17.054),
    (7.2182, 4.8852),
    (8.2951, 5.7442),
    (10.236, 7.7754),
    (5.4994, 1.0173),
    (20.341, 20.992),
    (10.136, 6.6799),
    (7.3345, 4.0259),
    (6.0062, 1.2784),
    (7.2259, 3.3411),
    (5.0269, -2.6807),
    (6.5479, 0.29678),
    (7.5386, 3.8845),
    (5.0365, 5.7014),
    (10.274, 6.7526),
    (5.1077, 2.0576),
    (5.7292, 0.47953),
    (5.1884, 0.20421),
    (6.3557, 0.67861),
    (9.7687, 7.5435),
    (6.5159, 5.3436),
    (8.5172, 4.2415),
    (9.1802, 6.7981),
    (6.002, 0.92695),
    (5.5204, 0.152),
    (5.0594, 2.8214),
    (5.7077, 1.8451),
    (7.6366, 4.2959),
    (5.8707, 7.2029),
    (5.3054, 1.9869),
    (8.2934, 0.14454),
    (13.394, 9.0551),
    (5.4369, 0.61705),
]
```

```{python}
import numpy as np
import matplotlib.pyplot as plt


# Convert data to numpy arrays
X = np.array([d[0] for d in data])
y = np.array([d[1] for d in data])

# Add intercept term to X
X = np.column_stack((np.ones(X.shape[0]), X))


def compute_cost(X, y, theta):
    m = len(y)
    predictions = X.dot(theta)
    cost = (1 / (2 * m)) * np.sum((predictions - y) ** 2)
    return cost


def gradient_descent(X, y, theta, alpha, iterations):
    m = len(y)
    cost_history = []

    for _ in range(iterations):
        predictions = X.dot(theta)
        theta = theta - (alpha / m) * X.T.dot(predictions - y)
        cost = compute_cost(X, y, theta)
        cost_history.append(cost)

    return theta, cost_history


# Initialize parameters
theta = np.zeros(2)
iterations = 1500
alpha = 0.01

# Run gradient descent
theta, cost_history = gradient_descent(X, y, theta, alpha, iterations)

# Print the final parameters
print(f"Final theta values: {theta}")

# Plotting the results
plt.figure(figsize=(10, 6))
plt.scatter(X[:, 1], y, color="red", marker="x", label="Training Data")
plt.plot(X[:, 1], X.dot(theta), color="blue", label="Linear Regression")
plt.xlabel("Population of City in 10,000s")
plt.ylabel("Profit in $10,000s")
plt.title("Food Truck Profit Prediction")
plt.legend()
plt.show()


# Function to predict profit
def predict_profit(population, theta):
    return theta[0] + theta[1] * population


# Example usage
population = 7.0
predicted_profit = predict_profit(population, theta)
print(
    f"For population = {population*10000}, predicted profit = ${predicted_profit*10000:.2f}"
)
```
```{python}

import math
import copy


def gradient_descent(
    x, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters
):
    """
    Performs batch gradient descent to learn theta. Updates theta by taking
    num_iters gradient steps with learning rate alpha

    Args:
      x :    (ndarray): Shape (m,)
      y :    (ndarray): Shape (m,)
      w_in, b_in : (scalar) Initial values of parameters of the model
      cost_function: function to compute cost
      gradient_function: function to compute the gradient
      alpha : (float) Learning rate
      num_iters : (int) number of iterations to run gradient descent
    Returns
      w : (ndarray): Shape (1,) Updated values of parameters of the model after
          running gradient descent
      b : (scalar)                Updated value of parameter of the model after
          running gradient descent
    """

    # number of training examples
    m = len(x)

    # An array to store cost J and w's at each iteration â€” primarily for graphing later
    J_history = []
    w_history = []
    w = copy.deepcopy(w_in)  # avoid modifying global w within function
    b = b_in

    for i in range(num_iters):

        # Calculate the gradient and update the parameters
        dj_dw, dj_db = gradient_function(x, y, w, b)

        # Update Parameters using w, b, alpha and gradient
        w = w - alpha * dj_dw
        b = b - alpha * dj_db

        # Save cost J at each iteration
        if i < 100000:  # prevent resource exhaustion
            cost = cost_function(x, y, w, b)
            J_history.append(cost)

        # Print cost every at intervals 10 times or as many iterations if < 10
        if i % math.ceil(num_iters / 10) == 0:
            w_history.append(w)
            print(f"Iteration {i:4}: Cost {float(J_history[-1]):8.2f}   ")

    return w, b, J_history, w_history  # return w and J,w history for graphing
```
```{python}

# initialize fitting parameters. Recall that the shape of w is (n,)
initial_w = 0.0
initial_b = 0.0

# some gradient descent settings
iterations = 1500
alpha = 0.01

w, b, _, _ = gradient_descent(
    x_train,
    y_train,
    initial_w,
    initial_b,
    compute_cost,
    compute_gradient,
    alpha,
    iterations,
)
print("w,b found by gradient descent:", w, b)

```
```{python}
def compute_multiple_features(x_1, x_2, x_3, x_4):
    w_1 = 0.1
    w_2 = 4
    w_3 = 10
    w_4 = -2
    b = 80
    return w_1 * x_1 + w_2 * x_2 + w_3 * x_3 + w_4 * x_4 + b

compute_multiple_features(1416, 3, 2, 40)

```

```{python}
import numpy as np


w = np.array([1.0, 2.5, -3.3])
b = 4
x = np.array([10, 20, 30])

f = w[0] * x[0] + w[1] * x[1] + w[2] * x[2] + b
print(f)

f = 0

for j in range(0, len(w)):
    f += w[j] * x[j]
f += b
print(f)

f = np.dot(w, x) + b
print(f)

```


